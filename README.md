# AI Chat Assistant with Sentry Instrumentation

A full-stack AI chat application built with React Native (frontend) and FastAPI (backend), featuring comprehensive error tracking and performance monitoring with Sentry.

## Features

- ğŸ¤– AI-powered chat with multiple providers (OpenAI, Groq, Google Gemini)
- ğŸ“± Native iOS/Android support
- ğŸ” Full Sentry instrumentation (frontend & backend)
- âš¡ Performance monitoring and tracing
- ğŸ“Š Langfuse LLM observability
- ğŸ¨ Modern UI with React Native

## Tech Stack

### Frontend
- React Native (Expo)
- TypeScript
- React Navigation
- Sentry React Native SDK

### Backend
- Python 3.9+
- FastAPI
- Sentry Python SDK
- Langfuse
- Multiple AI providers (OpenAI, Groq, Gemini)

## Getting Started

### Prerequisites

- Node.js 16+
- Python 3.9+
- iOS Simulator (for iOS development)
- Android Studio (for Android development)

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/serglom21/ai-chat-instrumentation.git
cd ai-chat-instrumentation
```

2. **Set up environment variables**

Frontend:
```bash
cp .env.example .env
# Edit .env and add your Sentry DSN
```

Backend:
```bash
cd backend
cp .env.example .env
# Edit .env and add your API keys and Sentry DSN
```

3. **Install dependencies**

Frontend:
```bash
npm install
```

Backend:
```bash
cd backend
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### Running the Application

1. **Start the backend**
```bash
cd backend
source venv/bin/activate
python run.py
```

2. **Start the frontend**

For Expo Go (quick start):
```bash
npm start
```

For native build (full Sentry features):
```bash
# iOS
LANG=en_US.UTF-8 npx expo run:ios

# Android
npx expo run:android
```

## Sentry Setup

### Frontend
1. Create a React Native project in Sentry
2. Copy the DSN to `.env` as `EXPO_PUBLIC_SENTRY_DSN`
3. For native builds: `npx expo prebuild`

### Backend
1. Create a Python project in Sentry
2. Copy the DSN to `backend/.env` as `SENTRY_DSN`

## Configuration

### AI Providers

Set in `backend/.env`:
```bash
AI_PROVIDER=groq  # Options: openai, groq, gemini
DEFAULT_MODEL=llama-3.3-70b-versatile
```

### Langfuse (Optional)

Add to `backend/.env`:
```bash
LANGFUSE_PUBLIC_KEY=your-key
LANGFUSE_SECRET_KEY=your-secret
LANGFUSE_HOST=https://us.cloud.langfuse.com
```

## Project Structure

```
.
â”œâ”€â”€ App.tsx                 # Frontend entry point with Sentry
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ screens/           # React Native screens
â”‚   â”œâ”€â”€ components/        # Reusable components
â”‚   â”œâ”€â”€ services/          # API services
â”‚   â””â”€â”€ types/             # TypeScript types
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py       # FastAPI app with Sentry
â”‚   â”‚   â”œâ”€â”€ routes.py     # API endpoints
â”‚   â”‚   â”œâ”€â”€ ai_service.py # AI provider integrations
â”‚   â”‚   â””â”€â”€ config.py     # Configuration
â”‚   â””â”€â”€ run.py            # Server entry point
â””â”€â”€ package.json
```

## License

MIT

## Contributing

Contributions are welcome! Please open an issue or submit a pull request.
